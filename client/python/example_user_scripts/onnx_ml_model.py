"""
Copyright (C) Microsoft Corporation. All rights reserved.

Demonstrates how to use an Onnx ML model on scene camera images.

The ONNX model file path is specified in the robot_quadrotor_fastphysics_onnx.jsonc
config as part of the camera sensor's settings called "post-process-model-settings".
"""

import asyncio

from projectairsim import ProjectAirSimClient, Drone, World
from projectairsim.utils import projectairsim_log

from PIL import Image
from matplotlib.colors import hsv_to_rgb
import numpy as np
import cv2

key = -1


class ImageProcessingWrapper(Drone):
    """
    Class to inherit the Drone class for nonphysics operations
    """

    cur_pose = None
    img_msg = None
    annotation_msg = None

    def callback_rgb_image(self, topic, image_msg):
        self.img_msg = image_msg

    def callback_onnx_ouput(self, topic, image_msg):
        self.annotation_msg = image_msg
        if self.img_msg is not None:
            display_annotated_image(self.img_msg, self.annotation_msg)

    def callback_actual_pose(self, topic, msg):
        self.cur_pose = msg


# This is the number of channels in the output from the ONNX model
num_classes = 21


def get_palette():
    # prepare and return palette
    palette = [0] * num_classes * 3

    for hue in range(num_classes):
        if hue == 0:  # Background color
            colors = (0, 0, 0)
        else:
            colors = hsv_to_rgb((hue / num_classes, 0.75, 0.75))

        for i in range(3):
            palette[hue * 3 + i] = int(colors[i] * 255)

    return palette


def colorize(labels):
    # generate colorized image from output labels and color palette
    result_img = Image.fromarray(labels).convert("P", colors=num_classes)
    result_img.putpalette(get_palette())
    return np.array(result_img.convert("RGB"))


def visualize_output(image, output):
    assert (
        image.shape[0] == output.shape[1] and image.shape[1] == output.shape[2]
    )  # Same height and width
    assert output.shape[0] == num_classes

    # get classification labels
    raw_labels = np.argmax(output, axis=0).astype(np.uint8)

    # comput confidence score
    confidence = float(np.max(output, axis=0).mean())

    # generate segmented image
    result_img = colorize(raw_labels)

    # generate blended image
    blended_img = cv2.addWeighted(image[:, :, ::-1], 0.5, result_img, 0.5, 0)

    return confidence, result_img, blended_img, raw_labels


# Use OpenCV directly to pop-up a sub-window for the camera sensor image
def display_annotated_image(image_msg, annotation_msg, win_name="Drone"):
    """Display the image using OpenCV HighGUI"""
    if image_msg is not None:
        o1 = np.frombuffer(image_msg["data"], dtype="uint8")
        ann = np.frombuffer(annotation_msg["data"], dtype="uint8")

        orig = np.reshape(o1, (image_msg["height"], image_msg["width"], 3))
        output = np.reshape(
            ann, (num_classes, annotation_msg["height"], annotation_msg["width"])
        )
        conf, result_img, blended_img, raw_labels = visualize_output(orig, output)
        cv2.imshow(win_name, blended_img)

        # Use OpenCV to capture keyboard input while sub-window is active
        global key
        key = cv2.waitKeyEx(20)


client = None
world = None
drone = None
private_assets = []


def connect_to_sim_server():
    global client, world, drone

    # Create a Project AirSim client and connect
    server = "127.0.0.1"
    client = ProjectAirSimClient(server)
    client.connect()
    projectairsim_log().info("**** Connected to Sim ****")

    # Create a World object to interact with the sim world
    world = World(client, "scene_onnx.jsonc", delay_after_load_sec=0)

    # Create a Drone object to interact with a drone in the sim world
    drone = ImageProcessingWrapper(client, world, "Drone1")


# Async main function to wrap async drone commands
async def main():
    try:
        # Connect to simulation environment
        connect_to_sim_server()

        # Subscribe to the actual scene camera which is being passed into the ONNX Model
        client.subscribe(
            drone.sensors["DownCamera"]["scene_camera"], drone.callback_rgb_image
        )

        # ------------------------------------------------------------------------------

        # Subscribe to the topic that publishes the output of the ONNX model
        # This callback overlays the result of the model on top of the acutal image
        client.subscribe(
            f"{drone.sensors_topic}/DownCamera/post_process_onnx_image",
            drone.callback_onnx_ouput,
        )

        # ------------------------------------------------------------------------------

        # Wait a bit for image data to start displaying at client
        await asyncio.sleep(1)

        # Set the drone to be ready to fly
        drone.enable_api_control()
        drone.arm()

        # Takeoff
        projectairsim_log().info("**** Takeoff started ****")
        meters_up = 10
        vel = 2.0
        cur_pos = drone.get_ground_truth_kinematics()["pose"]["position"]
        task = await drone.move_to_position_async(
            north=cur_pos["x"],
            east=cur_pos["y"],
            down=cur_pos["z"] - meters_up,
            velocity=vel,
        )

        await task

        # Command the Drone to move down / land
        projectairsim_log().info("land_async: starting")
        land_task = await drone.land_async()
        await land_task

        await asyncio.sleep(3)

        # Shut down the drone
        drone.disarm()
        drone.disable_api_control()

    except Exception as err:
        projectairsim_log().error(f"Exception occurred: {err}", exc_info=True)

    finally:
        # Always disconnect from the simulation environment to allow next connection
        client.disconnect()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    asyncio.run(main())  # Runner for async main function
